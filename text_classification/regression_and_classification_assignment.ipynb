{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42Im2753hpKY"
      },
      "source": [
        "# Regression and Classification Assignment\n",
        "\n",
        "This notebook uses the data discussed in regression_and_classification notebook.   You may want to review that if you\n",
        "have trouble with the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnUz9JCyhpKe"
      },
      "source": [
        "## 1. The data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuBbuIIVhpKf"
      },
      "source": [
        "First we do some imports and load the data from the `sklearn` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uD5g8xOJhpKg"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score,precision_score, recall_score\n",
        "from sklearn import linear_model\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "data = load_iris()\n",
        "features = data['data']\n",
        "feature_names = data['feature_names']\n",
        "target = data['target']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3hjVAj_hpKh"
      },
      "source": [
        "## 2.  The questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haxHzazGhpKi"
      },
      "source": [
        "**Q1**\n",
        "\n",
        "Don't take my word for it that subsets of the 4 Iris features always produce a worse classifier.\n",
        "Try it out.\n",
        "\n",
        "Change the selector array in the next cell and execute the code\n",
        "in the cell to try out your own favorite subsets\n",
        "of features.  No subset of 2 or even 3 features performs as well as using all 4.\n",
        "\n",
        "For example, using this selector\n",
        "```\n",
        "selector = np.array([True,True,False,True])\n",
        "```\n",
        "\n",
        "leaves out just  the third feature.  Here's your challenge: Find the best 2-feature\n",
        "system (it performs almost as well as any 3-feature system), and the best\n",
        "3-feature system.  Hint for the 2-feature problem, if you want to get there more\n",
        "quickly.  Look at our plots of two-feature systems in the \"Plotting 2D Projections of the data\"\n",
        "section of the regression_and_classification.ipynb notebook (Section 3).  Which one **looks**\n",
        "like it gives the best separation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PEgQIU9ghpKj",
        "outputId": "ff6afe42-8a62-4633-eb24-7232dc2637f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9666666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#Use selected features\n",
        "selector = np.array([True  ,True  ,False ,True])\n",
        "X = features[:,selector]\n",
        "#X = features\n",
        "Y = target\n",
        "logreg = linear_model.LogisticRegression(C=1e5, solver='lbfgs',multi_class='auto')\n",
        "logreg.fit(X, Y)\n",
        "\n",
        "#Tesing on training set.  Not usually done.\n",
        "predicted = logreg.predict(X)\n",
        "accuracy_score(Y,predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on looking at the graphs, the two-feature systems that generate the best separation are the bottom 3, so we will try each one. Also top middle.\n",
        "Feature 1 is sepal length, 2 is sepal width, 3 is petal length, 4 is petal width.\n",
        "\n",
        "Test Results:\n",
        "\n",
        "13: 0.967\n",
        "\n",
        "23: 0.953\n",
        "\n",
        "24: 0.953\n",
        "\n",
        "34: 0.96\n",
        "\n",
        "After finding the best 2 feature which was sepal and petal length, use that long with varying the other one to see.\n",
        "\n",
        "134: 0.98\n",
        "\n",
        "123: 0.967\n",
        "\n",
        "1234: 0.9867 which makes sense it would be the best since it has the most data points to have the best predictions\n",
        "\n",
        "\n",
        "Trying others:\n",
        "\n",
        "234: 0.98\n",
        "\n",
        "124: 0.967\n",
        "\n",
        "\n",
        "So the most accurate two-feature is sepal and petal length with an accuracy sccore of 0.967. The most accurate three-features were a tie between either sepal length or width along with both petal length and width.\n"
      ],
      "metadata": {
        "id": "D0oMws7SHkLE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRFngubohpKk"
      },
      "source": [
        "To give a clue as to how to code up a comparison of different classification\n",
        "models,  consider the following code, which uses 4 variant models\n",
        "and evaluates each model using the Accuracy metric.\n",
        "\n",
        "The SVC classifier with the RBF kernel is the winner, but not\n",
        "by much."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZI8n18ZfhpKl",
        "outputId": "693d0083-fb74-470d-dcb3-2fc41606ac61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9666666666666667,\n",
              " 0.9266666666666666,\n",
              " 0.9666666666666667,\n",
              " 0.9533333333333334]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from sklearn import svm, datasets\n",
        "\n",
        "#An SVM parameter, some others below.\n",
        "C=1.0\n",
        "\n",
        "# Cook up a tuple of models to test (These are the models from the last\n",
        "# plot in the simple regression NB)\n",
        "\n",
        "models = (svm.SVC(kernel='linear', C=C),\n",
        "          svm.LinearSVC(C=C, max_iter=10000),\n",
        "          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
        "          svm.SVC(kernel='poly', degree=3, gamma='auto', C=C))\n",
        "\n",
        "# train them Notice this is not a list comprehension but looks like one.\n",
        "models = (clf.fit(X, Y) for clf in models)\n",
        "\n",
        "#Testing on training set.  Not usually done.\n",
        "predicted = [model.predict(X) for model in models]\n",
        "accuracy = [accuracy_score(Y,prediction) for prediction in predicted]\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulXliK5HhpKm"
      },
      "source": [
        "One final point, this is balanced data, so it's not unreasonable to\n",
        "use accuracy to compare the 6 models, as in the cell above,\n",
        "but be sure to compare the accuracy over 10 different train/test splits as we did\n",
        "in Section 6 of the regression_and_classification notebook.  That means each of the 6 classifiers you're testing will generate 10 accuracy scores.  In order to declare a final winner, you should\n",
        "average the 10 accuracy scores.  The winner will be the classifier that achieves the highest average accuracy\n",
        "over the 10 test runs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmhScxIwhpKm"
      },
      "source": [
        "**Q2**\n",
        "\n",
        "In the regression_and_classification notebook,\n",
        "we implemented a function called `is_red` that is supposed to distinguish\n",
        "class 0 from classes 1 and 2.  Here is the definition and an example of using it\n",
        "on a row of the Iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uwLlT3sKhpKn",
        "outputId": "49e61f17-9ef3-4f04-c43c-aa94831f8003",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P is red: False\n"
          ]
        }
      ],
      "source": [
        "def is_red (P):\n",
        "    \"\"\"\n",
        "    P is a data point, all 4 attributes.\n",
        "\n",
        "    Return True if P is in positive class.\n",
        "    \"\"\"\n",
        "    return (P[2] - .9*P[0] + 2) < 0\n",
        "\n",
        "P = features[-90]\n",
        "print('P is red: {0}'.format(is_red(P)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_xbu-DZhpKn"
      },
      "source": [
        "We call such a function a model (a model of `redness` in this case).\n",
        "\n",
        "Consider a different model.  A model of non-redness. This model\n",
        "has a different positive class (non red instead of red).  We illustrate by applying to our\n",
        "model to example point P, which is not red:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iYltUROZhpKn",
        "outputId": "0fcda3d7-edc4-42bc-dff0-a1e2e1effc45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P is non red: True\n"
          ]
        }
      ],
      "source": [
        "def is_non_red (P):\n",
        "    \"\"\"\n",
        "    P is a data point, all 4 attributes.\n",
        "\n",
        "    Return True if P is in positive class.\n",
        "    \"\"\"\n",
        "    return (P[2] - .9*P[0] + 2) > 0\n",
        "\n",
        "P = features[-90]\n",
        "print('P is non red: {0}'.format(is_non_red(P)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5winVRmUhpKo"
      },
      "source": [
        "Write some code to evaluate this model's accuracy\n",
        "in predicting non-redness.  Before calling the `accuracy_score` function,\n",
        "you will have to compute `predicted` using `is_non_red` to give you an array of predictions\n",
        "\n",
        "\n",
        "To create a prediction array using `is_non_red`, apply it to every row\n",
        "of `features`:\n",
        "\n",
        "```\n",
        "np.apply_along_axis(is_non_red, axis=1, arr= features)\n",
        "```\n",
        "\n",
        "You will also have to evaluate accuracy against a different standard.\n",
        "You can't evaluate against the class array `target` defined above\n",
        "because that has three classes and `is_non_red` essentially works with two.\n",
        "You have to start by creating a different\n",
        "class array that only has two classes, matching the kind of results `is_non_red` produces.\n",
        "The red class is 0, so you want `True` for every class 1 or 2 iris, and\n",
        "`False` for every class 0 iris. For this, use a Boolean mask on `target`.\n",
        "\n",
        "For this problem, you do not have to different train/test splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJjg01AhhpKo",
        "outputId": "cc49fc0b-60dd-4779-cffc-db9419a50d69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.apply_along_axis(is_non_red, axis=1, arr= features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1KTwFBAhpKp"
      },
      "source": [
        "**Q3  Visualization**\n",
        "\n",
        "Using the `rock vs. mines` data set loaded in the next cell, draw a scatterplot of the points like the ones we did in Section 2 and 3 of the regression_and_classification notebook (R&C NB) for the iris data.\n",
        "You will find the code in Section 3 of the R&C NB will require the least modification.\n",
        "\n",
        "This data set tries to distinguish two classes, rocks and mines, based on sonar readings.\n",
        "As the loading code indicates, this data has 60 features, which is a lot, so you should\n",
        "just pick one pair of features out and plot them.  Crucially,\n",
        "you **don't** need to plot all pairs (there are 1770 pairs).\n",
        "\n",
        "When modifying the code to draw the picture, bear in mind you really only need to\n",
        "scatter the points.  You are not being asked to draw a separation line,\n",
        "and you don't need to draw the point P being depicted in Sections 2 and 3,\n",
        "because it doesn't appear in this data.\n",
        "\n",
        "You should try to give the points belonging to the two classes different colors\n",
        "and shapes, as was done in R&C NB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "le_V-M95hpKp",
        "outputId": "3fd724c2-4554-4627-ca0f-94d3cef91757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "208 exemplars with 61 features\n",
            "2 classes: ['M', 'R']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "target_url = (\"https://archive.ics.uci.edu/ml/machine-learning-\"\n",
        "\"databases/undocumented/connectionist-bench/sonar/sonar.all-data\")\n",
        "\n",
        "#read rocks mines data\n",
        "mine_df = pd.read_csv(target_url, header=None), prefix=\"V\")\n",
        "#mine_df = pd.read_csv(target_url, header=None)\n",
        "mine_features = mine_df.values\n",
        "(samp_sz, num_feats) = mine_features.shape\n",
        "print('{0} exemplars with {1} features'.format(samp_sz, num_feats))\n",
        "# The last column contains the class data.  Separate it from the others.\n",
        "target = mine_features[:, 60]\n",
        "# Everything else is a feature\n",
        "mine_features = mine_features[:, :60]\n",
        "(samp_sz, num_feats) = mine_features.shape\n",
        "\n",
        "# Find the two classes we're trying to separate.\n",
        "feat_set = list(set(target))\n",
        "print('{0} classes: {1}'.format(len(feat_set), feat_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCpIIiEXhpKp",
        "outputId": "9f4c201a-b91a-428f-8811-e401fd32e6e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 'o', 'r'), (1, 'x', 'g')]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "markers,colors = 'ox','rg'\n",
        "list(zip(range(3),markers,colors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hugm-u-thpKq",
        "outputId": "deeeebdb-9edd-46b0-a87d-2182d836f4a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([], dtype=object)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mine_features[target==0,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDyn-pLyhpKq",
        "outputId": "8a9659cc-d299-4edb-bc83-790744a2b02e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 2]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(range(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhRqPCYShpKr"
      },
      "source": [
        "**Q4**\n",
        "\n",
        "Combining variables: Cheap non-linearity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKVLrcg3hpKr"
      },
      "source": [
        "The [Heidelberg stat-center](http://www.statlab.uni-heidelberg.de/data/iris/) reports a very nice result obtained by combining\n",
        "the 2 petal features and the  2 sepal features by multiplication.  We end up with 2 **area** features, and a system that's almost as good as a 4-feature linear system,\n",
        "at the cost of a little non-linearity (the multiplication). Here's the computation,\n",
        "and the picture, both of which are very simple.  \n",
        "\n",
        "Notice what the picture is telling us:  Petal area alone\n",
        "is enough to almost perfectly perform the discrimination.  And the insight can easily be made visual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "X_yr2prehpKs",
        "outputId": "bce30452-5095-4895-abbe-ce6bbaeaa2b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'petal area')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "data = load_iris()\n",
        "features = data['data']\n",
        "feature_names = data['feature_names']\n",
        "target = data['target']\n",
        "sepal_area = features[:,0] * features[:,1]\n",
        "petal_area = features[:,2] * features[:,3]\n",
        "\n",
        "for t,marker,c in zip(list(range(3)),\">ox\",\"rgb\"):\n",
        "    plt.scatter(sepal_area[target == t], petal_area[target == t], marker=marker, c=c,s=60)\n",
        "\n",
        "# Let's draw our point P with extra special attention getting large SIZE.\n",
        "p_sepal_area,p_petal_area,p_target  = sepal_area[-90],petal_area[-90],target[-90]\n",
        "marker,clr = \">ox\"[p_target], \"rgb\"[p_target]\n",
        "plt.scatter(p_sepal_area,p_petal_area,marker=marker,c=clr,s=180)\n",
        "\n",
        "# Let's draw some linear separators, horizontal lines in this case\n",
        "plt.plot(np.arange(0.,40.),np.array([1.8]*40))\n",
        "plt.plot(np.arange(0.,40.),np.array([7.4]*40))\n",
        "plt.axis('tight')\n",
        "plt.xlabel('sepal area')\n",
        "plt.ylabel('petal area')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw6S6G6whpKs"
      },
      "source": [
        "**Q4: Part A**\n",
        "\n",
        "Validate what the picture is telling you by training and evaluating a `linear_model.LogisticRegression`\n",
        "classifier that uses **only** the petal area feature (the main\n",
        "dimension of separation in the picture).   Be sure to do a training/test split and\n",
        "use 20% of the data for your test, as in\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into a training set and a test set (random_state=0 guarantees the same\n",
        "# split on multiple runs)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=47)\n",
        "```\n",
        "\n",
        "For this problem, you do not have to do multiple train/test runs.\n",
        "\n",
        "You can define values for `petal_area` feature by evaluating the cell above.  Your\n",
        "answer can rely on that definition\n",
        "and therefore should be only a few lines of code.  The last few lines should print out\n",
        "precision, recall, and accuracy scores for your classifier.\n",
        "\n",
        "For submitting your discussion question answers, you will want to\n",
        "submit the few lines of code plus some evaluation results. For those,\n",
        "consider the following points.\n",
        "\n",
        "1.  When we defined precision and recall, we did so in a setting where there\n",
        "    were only two classes.  For this problem there are three classes, which\n",
        "    means you will need to choose between different interpretation of\n",
        "    precision and recall.  Use `average=None`, as in\n",
        "\n",
        "    ```python\n",
        "    metrics.precision_score(actual,predicted,average=None)\n",
        "    ```\n",
        "\n",
        "    This means you will get three different precision scores,\n",
        "    one for each species, instead of a single averaged score.\n",
        "\n",
        "2. You may also find it interesting to try out the scikit learn confusion matrix:\n",
        "\n",
        "   ```python\n",
        "   from sklearn import metrics\n",
        "   print(metrics.confusion_matrix(actual, predicted))\n",
        "   ```\n",
        "\n",
        "   For a slightly different display strategy, see the section entitled **An evaluation experiment** in the [Classification notebook.](https://colab.research.google.com/github/gawron/python-for-social-science/blob/master/text_classification/regression_and_classification.ipynb)\n",
        "   You might also want to try the variant `multilabel_confusion_matrix` appropriate for multiclass problems,\n",
        "   which returns a 3D matrix, one layer for each class, where each layer is a 2x2 (one vs all) confusion matrix\n",
        "   for its class.\n",
        "\n",
        "3.  Finally, for an informative (but not complete) report on your classifier, try\n",
        "\n",
        "    ```python\n",
        "    print(metrics.classification_report(actual, predicted, digits=3))\n",
        "    ```\n",
        "\n",
        "    which also gives support figures --- how many instances of each class were found in the data --- and F-score, a way of averaging the precision and recall scores.\n",
        "    \n",
        "    By default, `classification_report` returns a formatted string which can be printed\n",
        "    as done here. With the optional argument `output_dict=True`, it returns a dictionary,\n",
        "    which you can do more with, such as creating a pandas `DataFrame` using `pd.DataFrame.from_dict`.\n",
        "    This gives you more control over what things are displayed and how.\n",
        "    \n",
        "    An alternative way of getting at the numbers you want is\n",
        "    \n",
        "    ```python\n",
        "    (p,r,f,s) = metrics.precision_recall_fscore_support(Y,predicted,average=None)\n",
        "    ```\n",
        "    \n",
        "    Again, it would make sense to store these numbers in a DataFrame.  \n",
        "    \n",
        "Motivational note:  What we are doing in this problem is pejoratively referred to\n",
        "as **feature hacking**, but really it's another example of\n",
        "the sort of transformations of the data into a different\n",
        "space that we explored when we looked at polynomial\n",
        "models in the simple regression notebook.  What does multiplying feature values have to do with\n",
        "higher-dimensional polynomial models?  After all,\n",
        "we started with 4 features and now have only 1!  Well of course\n",
        "the petal area model is not a higher dimensional model, but it is a subset of the features of a quadratic model\n",
        "with feature interactions.  Suppose we have a 2-dimensional model:\n",
        "\n",
        "$$\n",
        "\\lbrack a, b  \\rbrack\n",
        "$$\n",
        "\n",
        "The quadratic model with feature interactions would transform this into a 6-dimensional model\n",
        "\n",
        "$$\n",
        "\\lbrack 1, a, b, ab, a^{2}, b^{2}  \\rbrack\n",
        "$$\n",
        "\n",
        "In scikit learn, the feature `ab` is called an interaction feature.\n",
        "Hence our proposed petal area model is like a quadratic model\n",
        "that uses only the interaction feature.  You can't\n",
        "quite get that using `preprocessing.PolynomialFeatures` in\n",
        "scikit learn, but you can get close.\n",
        "If you start with a 2-feature model (`a` and `b`), the minimal quadratic\n",
        "transformer in scikit learn is constructed as follows:\n",
        "\n",
        "```python\n",
        "poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
        "\n",
        "```\n",
        "\n",
        "This produces a model with 3 features:\n",
        "\n",
        "$$\n",
        "\\lbrack a, b, ab \\rbrack\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsRk5k7FhpKt"
      },
      "source": [
        "**More challenging version of Q4 Part A**: Instead of the 1-feature petal area representation the first\n",
        "sentence of Q4 Part A describes, you are welcome to try the following slightly\n",
        "more challenging strategy in answering question Q4 Part A.\n",
        "\n",
        ">Use scikit learn's `PolynomialFeatures`\n",
        "transformer (created with the parameters above)\n",
        "to represent your training and test data\n",
        "instead of the `petal_area` representation created in the code cell for Q4.\n",
        "To get the fewest number of features, you\n",
        "would submit just the petal features (columns 3 and 4 in `features`)\n",
        "to the transformer.\n",
        "\n",
        "This will give you a 3-feature representation\n",
        "of the data. That's because petal length and petal width will be kept\n",
        "as features, but that will make little\n",
        "difference to the structure of your code.  You can do this either by\n",
        "transforming the data directly using the transformer's `.fit_transform()` and\n",
        "`.transform()` methods or by using the sort of pipeline that was demonstrated in the polynomial model\n",
        "in the simple_regression notebook. Whether you use the pipeline or\n",
        "not, you may find it helpful to include scaling\n",
        "in processing the data (as was done in the simple_regression notebook)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yHhtWqQghpKt",
        "outputId": "f6a251f5-f493-4153-e312-8729143e88b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sepal length (cm)',\n",
              " 'sepal width (cm)',\n",
              " 'petal length (cm)',\n",
              " 'petal width (cm)']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "feature_names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(petal_area.reshape(-1, 1), target, test_size=.2, random_state=69)\n",
        "lin = linear_model.LogisticRegression(C=1e5, solver='liblinear',multi_class='auto')\n",
        "lin.fit(X_train, y_train)\n",
        "predicted = lin.predict(X_test)\n",
        "\n",
        "actual = y_test\n",
        "metrics.precision_score(actual,predicted,average=None)\n",
        "\n",
        "from sklearn import metrics\n",
        "print(metrics.confusion_matrix(actual, predicted))\n",
        "print(metrics.classification_report(actual, predicted, digits=3))"
      ],
      "metadata": {
        "id": "FVQOJF2QSu-u",
        "outputId": "287fb306-bc4f-4ab2-9cd9-df990b1bc8df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[10  0  0]\n",
            " [ 0  8  0]\n",
            " [ 0  1 11]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     1.000     1.000        10\n",
            "           1      0.889     1.000     0.941         8\n",
            "           2      1.000     0.917     0.957        12\n",
            "\n",
            "    accuracy                          0.967        30\n",
            "   macro avg      0.963     0.972     0.966        30\n",
            "weighted avg      0.970     0.967     0.967        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSZ0QkjZhpKt"
      },
      "source": [
        "**Q4: Part B**\n",
        "\n",
        "Train and evaluate a `linear_model.LogisticRegression`\n",
        "classifier that uses **both** the petal area feature and the sepal area feature.  \n",
        "Guidelines as with the last problem, except you are asked  not to use scikit learn's `PolynomialFeatures`\n",
        "for this one.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_sepalpetal = np.concatenate([petal_area.reshape(-1, 1), sepal_area.reshape(-1, 1)], axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sepalpetal, target, test_size=.2, random_state=69)\n",
        "lin = linear_model.LogisticRegression(C=1e5, solver='liblinear',multi_class='auto')\n",
        "lin.fit(X_train, y_train)\n",
        "predicted = lin.predict(X_test)\n",
        "\n",
        "actual = y_test\n",
        "metrics.precision_score(actual,predicted,average=None)\n",
        "\n",
        "from sklearn import metrics\n",
        "print(metrics.confusion_matrix(actual, predicted))\n",
        "print(metrics.classification_report(actual, predicted, digits=3))"
      ],
      "metadata": {
        "id": "hlLz1cVWa4Hj",
        "outputId": "fe8a6a29-de47-4a84-e180-771083ac8d5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[10  0  0]\n",
            " [ 0  8  0]\n",
            " [ 0  0 12]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     1.000     1.000        10\n",
            "           1      1.000     1.000     1.000         8\n",
            "           2      1.000     1.000     1.000        12\n",
            "\n",
            "    accuracy                          1.000        30\n",
            "   macro avg      1.000     1.000     1.000        30\n",
            "weighted avg      1.000     1.000     1.000        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate petal_area and sepal_area horizontally\n",
        "X_combined = np.concatenate([features[].reshape(-1, 1), sepal_area.reshape(-1, 1)], axis=1)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "precision = metrics.precision_score(y_test, y_pred, average=None)\n",
        "recall = metrics.recall_score(y_test, y_pred, average=None)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print the confusion matrix\n",
        "conf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate and print the multilabel confusion matrix\n",
        "multilabel_conf_matrix = metrics.multilabel_confusion_matrix(y_test, y_pred)\n",
        "print(\"Multilabel Confusion Matrix:\")\n",
        "print(multilabel_conf_matrix)"
      ],
      "metadata": {
        "id": "nCDN930ibpO2",
        "outputId": "21620a8d-5e94-4618-fa3a-87574a1bdf51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: [1. 1. 1.]\n",
            "Recall: [1. 1. 1.]\n",
            "Accuracy: 1.0\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  0]\n",
            " [ 0  0 12]]\n",
            "Multilabel Confusion Matrix:\n",
            "[[[20  0]\n",
            "  [ 0 10]]\n",
            "\n",
            " [[22  0]\n",
            "  [ 0  8]]\n",
            "\n",
            " [[18  0]\n",
            "  [ 0 12]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names"
      ],
      "metadata": {
        "id": "SdzFDNDrlt6w",
        "outputId": "3bdf7d93-c9bc-45ee-c322-0c738a4449c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sepal length (cm)',\n",
              " 'sepal width (cm)',\n",
              " 'petal length (cm)',\n",
              " 'petal width (cm)']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "IxsPn0GMhpKt"
      },
      "source": [
        "**Q4 Part C**\n",
        "\n",
        "Train and evaluate a `linear_model.LogisticRegression`\n",
        "classifier that uses both  area features **as well as** the original 4 features.\n",
        "That's a 6 feature model, the 4 original features plus two\n",
        "interaction features.  Does this do better than a classifier trained on just the original 4 features?\n",
        "Guidelines as with Part A, except that you are asked noto to use scikit learn's PolynomialFeatures for this one."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##add all 4 features\n",
        "\n",
        "X_allsix = np.concatenate([features, X_combined], axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_allsix, target, test_size=.2, random_state=69)\n",
        "lin = linear_model.LogisticRegression(C=1e5, solver='liblinear',multi_class='auto')\n",
        "lin.fit(X_train, y_train)\n",
        "predicted = lin.predict(X_test)\n",
        "\n",
        "actual = y_test\n",
        "metrics.precision_score(actual,predicted,average=None)\n",
        "\n",
        "from sklearn import metrics\n",
        "print(metrics.confusion_matrix(actual, predicted))\n",
        "print(metrics.classification_report(actual, predicted, digits=3))"
      ],
      "metadata": {
        "id": "JWUe_uIfbwd9",
        "outputId": "e828e18c-c225-467c-9bbe-4db5dc407e87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[10  0  0]\n",
            " [ 0  8  0]\n",
            " [ 0  0 12]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     1.000     1.000        10\n",
            "           1      1.000     1.000     1.000         8\n",
            "           2      1.000     1.000     1.000        12\n",
            "\n",
            "    accuracy                          1.000        30\n",
            "   macro avg      1.000     1.000     1.000        30\n",
            "weighted avg      1.000     1.000     1.000        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names"
      ],
      "metadata": {
        "id": "Gk8k1I2oiIqH",
        "outputId": "274e68e8-3289-44f8-aae3-f660d4013e7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sepal length (cm)',\n",
              " 'sepal width (cm)',\n",
              " 'petal length (cm)',\n",
              " 'petal width (cm)']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMv2oQRbhpKu"
      },
      "source": [
        "**Q4 Part D**\n",
        "\n",
        "Train and evaluate a `linear_model.LogisticRegression`\n",
        "classifier that uses a 2-feature PCA representation of the iris data,\n",
        "as was done in Section 7 of the Regression and Classification notebook.\n",
        "Does the PCA classifier do better than the 2-feature classifier\n",
        "models evaluated in Q1 of this assignment?  For\n",
        "this comparison, you should split the data into training and test\n",
        "sets 10 times and average the precision scores, as you\n",
        "did for Q1.\n",
        "\n",
        "The other question that arises is how this\n",
        "2-feature model compares with the 2-feature model\n",
        "you evaluated in Q4 Part B.  That evaluation\n",
        "scheme was slightly different, however,\n",
        "since we didn't do multiple train/test splits.\n",
        "To provide a fair comparison, redo your evaluation\n",
        "of the Part B system using 10 train/test\n",
        "splits and computing the average accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#Best 2 features from Q1 are sepal and petal length\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "c_length = np.concatenate([features[:,0].reshape(-1,1), features[:,2].reshape(-1,1)], axis=1) #first one is sepal length and second is petal length\n",
        "X = c_length\n",
        "def multiple_runs_reducer(X,y,num_runs,reducer):\n",
        "    accs = np.zeros(num_runs)\n",
        "    for i in range(num_runs):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(c_length, target, test_size=.2,random_state=i)\n",
        "        #reducer = dec.PCA(n_components=2)\n",
        "        X_train_reduced = reducer.fit_transform(X_train)\n",
        "        # Use the transform method of the trained reducer on the test data\n",
        "        # No fitting allowed on test data\n",
        "        X_test_reduced = reducer.transform(X_test)\n",
        "        lin = linear_model.LogisticRegression(C=1e5, solver='liblinear',multi_class='auto')\n",
        "        lin.fit(X_train_reduced, y_train)\n",
        "        predicted = lin.predict(X_test_reduced)\n",
        "        acc = accuracy_score(y_test,predicted)\n",
        "        accs[i] = acc\n",
        "        # Evaluate\n",
        "        print(f\"{acc:>6.1%}\")\n",
        "    return accs\n",
        "\n",
        "#get average\n",
        "accs_pca = multiple_runs_reducer(X,y,10,reducer=dec.PCA(n_components=2))\n",
        "#get average of runs\n",
        "average = np.mean(accs_pca)\n",
        "print(f\"Average = {average:.3f}\")"
      ],
      "metadata": {
        "id": "KJR9C7nljMph",
        "outputId": "24d1d77a-e2b7-4f2c-a2e9-30ca97aa16b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 96.7%\n",
            " 96.7%\n",
            " 96.7%\n",
            " 93.3%\n",
            " 96.7%\n",
            " 93.3%\n",
            " 96.7%\n",
            " 86.7%\n",
            " 96.7%\n",
            "100.0%\n",
            "Average = 0.953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing regular regression without PCA for the same ones\n",
        "def multiple_runs(X,y,num_runs):\n",
        "  accs = np.zeros(num_runs)\n",
        "  for i in range(num_runs):\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(c_length, target, test_size=.2, random_state=i)\n",
        "    lin = linear_model.LogisticRegression(C=1e5, solver='liblinear',multi_class='auto')\n",
        "    lin.fit(X_train, y_train)\n",
        "    predicted = lin.predict(X_test)\n",
        "    acc = accuracy_score(y_test,predicted)\n",
        "    accs[i] = acc\n",
        "    # Evaluate\n",
        "    print(f\"{accuracy_score(y_test,predicted):>6.1%}\",\n",
        "          )\n",
        "\n",
        "  average_accuracy = np.mean(accs)\n",
        "  print(f\"\\nAverage Accuracy: {average_accuracy:.1%}\")\n",
        "\n",
        "  return accs\n",
        "\n",
        "#get average\n",
        "accs_norm = multiple_runs(X,y,10)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-fk4k37UncDN",
        "outputId": "673afcf5-9384-4b5b-e3f4-bedc9809696c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 96.7%\n",
            " 96.7%\n",
            " 96.7%\n",
            " 93.3%\n",
            " 96.7%\n",
            " 93.3%\n",
            " 96.7%\n",
            " 86.7%\n",
            " 96.7%\n",
            "100.0%\n",
            "\n",
            "Average Accuracy: 95.3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X and y are your feature matrix and target variable\n",
        "\n",
        "def multiple_runs(X, y, num_runs):\n",
        "    accs = np.zeros(num_runs)\n",
        "    for i in range(num_runs):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "        lin = LogisticRegression(C=1e5, solver='liblinear', multi_class='auto')\n",
        "        lin.fit(X_train, y_train)\n",
        "        predicted = lin.predict(X_test)\n",
        "        acc = accuracy_score(y_test, predicted)\n",
        "        accs[i] = acc\n",
        "        # Evaluate\n",
        "        print(f\"Iteration {i + 1}: Accuracy = {acc:.1%}\")\n",
        "\n",
        "    # Calculate and print the average accuracy\n",
        "    average_accuracy = np.mean(accs)\n",
        "    print(f\"\\nAverage Accuracy: {average_accuracy:.1%}\")\n",
        "\n",
        "    return accs\n",
        "\n",
        "# Example usage\n",
        "num_runs = 10\n",
        "accuracy_scores = multiple_runs(c_length, target, num_runs)"
      ],
      "metadata": {
        "id": "CZaO2vjJq8gp",
        "outputId": "61351a39-0147-4312-efb2-1d9e33523227",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Accuracy = 96.7%\n",
            "Iteration 2: Accuracy = 96.7%\n",
            "Iteration 3: Accuracy = 96.7%\n",
            "Iteration 4: Accuracy = 93.3%\n",
            "Iteration 5: Accuracy = 96.7%\n",
            "Iteration 6: Accuracy = 93.3%\n",
            "Iteration 7: Accuracy = 96.7%\n",
            "Iteration 8: Accuracy = 86.7%\n",
            "Iteration 9: Accuracy = 96.7%\n",
            "Iteration 10: Accuracy = 100.0%\n",
            "\n",
            "Average Accuracy: 95.3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = c_length\n",
        "for i in range(10):\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
        "    lin = linear_model.LogisticRegression(C=1e5, solver='liblinear',multi_class='auto')\n",
        "    lin.fit(X_train, y_train)\n",
        "    predicted = lin.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predicted)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    # Evaluate\n",
        "    print(f\"{accuracy_score(y_test,predicted):>6.1%}\",\n",
        "          )\n",
        "\n",
        "# Calculate and print the average accuracy\n",
        "average_accuracy = np.mean(accuracy_scores)\n",
        "print(f\"\\nAverage Accuracy: {average_accuracy:.1%}\")"
      ],
      "metadata": {
        "id": "79-DquF-qECe",
        "outputId": "0d0c05eb-3b81-47de-aa48-d417f7200775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-43e28ae4f3b5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0maccuracy_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     print(f\"{accuracy_score(y_test,predicted):>6.1%}\",\n",
            "\u001b[0;31mNameError\u001b[0m: name 'accuracy_scores' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names"
      ],
      "metadata": {
        "id": "fxbbYSLnl-69",
        "outputId": "dd7261bd-1cbe-4952-8e19-9940bac12818",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sepal length (cm)',\n",
              " 'sepal width (cm)',\n",
              " 'petal length (cm)',\n",
              " 'petal width (cm)']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features[:,0]"
      ],
      "metadata": {
        "id": "W43nAjndmGrD",
        "outputId": "8ae12a1b-cbde-4632-d262-a13e0e44e606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.1, 4.9, 4.7, 4.6, 5. , 5.4, 4.6, 5. , 4.4, 4.9, 5.4, 4.8, 4.8,\n",
              "       4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1, 5.4, 5.1, 4.6, 5.1, 4.8, 5. ,\n",
              "       5. , 5.2, 5.2, 4.7, 4.8, 5.4, 5.2, 5.5, 4.9, 5. , 5.5, 4.9, 4.4,\n",
              "       5.1, 5. , 4.5, 4.4, 5. , 5.1, 4.8, 5.1, 4.6, 5.3, 5. , 7. , 6.4,\n",
              "       6.9, 5.5, 6.5, 5.7, 6.3, 4.9, 6.6, 5.2, 5. , 5.9, 6. , 6.1, 5.6,\n",
              "       6.7, 5.6, 5.8, 6.2, 5.6, 5.9, 6.1, 6.3, 6.1, 6.4, 6.6, 6.8, 6.7,\n",
              "       6. , 5.7, 5.5, 5.5, 5.8, 6. , 5.4, 6. , 6.7, 6.3, 5.6, 5.5, 5.5,\n",
              "       6.1, 5.8, 5. , 5.6, 5.7, 5.7, 6.2, 5.1, 5.7, 6.3, 5.8, 7.1, 6.3,\n",
              "       6.5, 7.6, 4.9, 7.3, 6.7, 7.2, 6.5, 6.4, 6.8, 5.7, 5.8, 6.4, 6.5,\n",
              "       7.7, 7.7, 6. , 6.9, 5.6, 7.7, 6.3, 6.7, 7.2, 6.2, 6.1, 6.4, 7.2,\n",
              "       7.4, 7.9, 6.4, 6.3, 6.1, 7.7, 6.3, 6.4, 6. , 6.9, 6.7, 6.9, 5.8,\n",
              "       6.8, 6.7, 6.7, 6.3, 6.5, 6.2, 5.9])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOkTRzrOhpKu"
      },
      "source": [
        "**Q5**\n",
        "\n",
        "Another day, another model.  This question is completely optional. No\n",
        "extra credit for doing it, just glory.\n",
        "\n",
        "We looked at one nonlinear model with regression (K Nearest Neighbor)\n",
        "and we'll look at one nonlinear model with classification.\n",
        "This one is called a **Gaussian Process Classifier**.\n",
        "This is a probabilistic model that tries to fit a Gaussian\n",
        "mixture, so in our setting, the trained classifier\n",
        "will assign 3 probabilities to each row in the data,\n",
        "one for each class.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a6j0ksJhpKu",
        "outputId": "59c84927-4a8f-4935-d33b-c91e3dd5e92f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9866666666666667\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "newX, y = load_iris(return_X_y=True)\n",
        "\n",
        "kernel = 1.0 * RBF(1.0)\n",
        "gpc = GaussianProcessClassifier(kernel=kernel,\n",
        "                                random_state=0).fit(newX, y)\n",
        "print(gpc.score(newX, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgIoCCvthpKu"
      },
      "source": [
        "As the code shows, this classifier needs a **kernel**.  For\n",
        "a discussion of kernels and classifiers, see [The SVM Classifier notebook.](https://colab.research.google.com/github/gawron/python-for-social-science/blob/master/text_classification/linear_classifier_svm.ipynb)\n",
        "For more discussion of Gaussian Process Classifiers and a visualization of what they do\n",
        "on the iris data, see [The Plot Gaussian Process notebook.](https://colab.research.google.com/github/gawron/python-for-social-science/blob/master/text_classification/plot_gaussian_process_iris.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofDlr3MJhpKv"
      },
      "source": [
        "To demonstrate the idea of assigning three probabilities to each row, let's pass the first two rows\n",
        "of the training data to the `predict_proba` method of the trained classifier (only probabilistic classifiers have a `predict_proba` method):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bchCaxzHhpKv",
        "outputId": "382dd224-98f1-4ab9-fda5-7bbf4b305014"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.83548752, 0.03228706, 0.13222543],\n",
              "       [0.79064206, 0.06525643, 0.14410151]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prob_array = gpc.predict_proba(newX[:2,:])\n",
        "#array([[0.8354..., 0.03228..., 0.1322...],\n",
        "#       [0.7906..., 0.0652..., 0.1441...]])\n",
        "prob_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoJsMm2_hpKv"
      },
      "source": [
        "We get a 2x3 array.  Row 0 of `prob_array` represents the probabilities for the 3 classes for\n",
        "the iris represented in row 0 of the training data (`newX`).  For example the probability that\n",
        "that iris belongs to class 1  (versicolor) is around `.03`  and the probability that it belongs to\n",
        "class 2 (virginica) is around `.13`.  So the class with the greatest probability is class 0 (setosa),\n",
        "which is actually the  correct class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvhYiW0jhpKv",
        "outputId": "86f432ae-214e-4552-91c4-b12391d6e26e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['setosa' 'versicolor' 'virginica']\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(data['target_names'])\n",
        "print(target[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JezNhvT0hpKw"
      },
      "source": [
        "Of course the classifier also has a `.predict()` method, so we can just get an array of decisions, as with\n",
        "any scikit learn classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wi6hu1fhpKw",
        "outputId": "3f1a224f-15da-48b7-ae55-a3b3505e19ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpc.predict(newX[:2,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9TWm0hZhpK2"
      },
      "source": [
        "The default decision rule is to choose the the maximum probability in any row; if we take the max\n",
        "probabilities in each row and look at their mean, it's quite high, meaning\n",
        "the classifier is for the most part pretty confident about its selections, at least for the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDyp94wLhpK2",
        "outputId": "69953a5d-2a86-4a40-fe50-2dc413eff78b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7568465696129202"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pc = gpc.predict_proba(newX)\n",
        "pc.max(axis=1).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpAKOA1YhpK3"
      },
      "source": [
        "But in at least a few case it is deciding by, in effect, flipping a three-sided coin. The minimally confident training classification is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laZpel4FhpK3",
        "outputId": "85f9b7cc-ae8f-42d2-ac1f-0b95f76253a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.4096513245830137"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pc.max(axis=1).min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPM22TndhpK3"
      },
      "source": [
        "This means that according to the system's own probability model, the max probability guess for this particular iris has a better than even chance of being wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtMG2Xp1hpK3"
      },
      "source": [
        "A more principled  measure of the classifier's overall fit to the training data is the log marginal likelihood,\n",
        "which is found on the trained kernel.  \n",
        "\n",
        "It is the log of a marginal probability, so higher is better. The actual formula (see the scikit learn docs) is  slightly complicated, partly because the formal definition involves integrals with no closed-form solution,\n",
        "and approximations are used, and partly because the underlying logic of the model is Bayesian, so\n",
        "there is a penalty for \"model complexity\" built into the probability (more complex models are less\n",
        "probable, a form of Occam's razor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wfa3oZT3hpK4",
        "outputId": "1c1ba43f-6da5-4252-9328-546dcd40b296"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-13.73091847787589"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpc.log_marginal_likelihood(gpc.kernel_.theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_cVyhX7hpK4"
      },
      "source": [
        "Converted to a probability this is quite low, but it is associated with the entire data set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yCwISJEhpK4",
        "outputId": "6112da0d-2c60-484b-c893-baf152b8f2d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0882730478969875e-06"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.exp(gpc.log_marginal_likelihood(gpc.kernel_.theta))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwwW793zhpK4",
        "outputId": "22630397-c190-48e8-f131-2707deeed4cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9125253103692804"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# converted to a per training item prob\n",
        "np.exp(gpc.log_marginal_likelihood(gpc.kernel_.theta))**(1/len(newX))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNyhSehThpK4"
      },
      "source": [
        "Your task in this problem is to build a classifer model using the 2-feature representation\n",
        "used in question 4b, and to decide which of two kernels is better, the one used above\n",
        "(Isotropic Area RBF) or a 2D option available for a 2-feature model:  1.0 * RBF([1.0, 1.0]).\n",
        "\n",
        "```python\n",
        "\n",
        "Isotropic Area RBF    1.0 * RBF(1.0)\n",
        "Anisotropic Area RBF  1.0 * RBF([1.0, 1.0])\n",
        "```\n",
        "\n",
        "You should evaluate using accuracy, precision, and recall (with `average=None` as with previous evaluations).\n",
        "You should also compare the log_marginal_likelihood of the two models.\n",
        "\n",
        "Warnings Warning:  You may get warnings when you try this.  They are mostly there\n",
        "to tell you the model is not optimal, and a better model might be lying in wait.  But persevere. You will still get usable results.\n",
        "\n",
        "Extra glory:  Do 10 test runs and compare the two systems for average accuracy, precision, and recall\n",
        "over those 10 runs.  Hint: store your results  in arrays. For an example of how to do this\n",
        "for accuracy have a look at the `multiple_runs(X,y,num_runs)` function defined in Section 7\n",
        "of the Regression and Classification notebook.  The way to get the mean of the numerical values in a\n",
        "1D array `a`  is `a.mean()`.  Note that precision and recall present an extra headache in this\n",
        "multiclass setting, since (unlike accuracy) they will give you three scores for each system run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIRL1aF9hpK5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "171px",
        "width": "253px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}